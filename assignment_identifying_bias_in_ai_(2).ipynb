{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavrangraves/Hello_world/blob/master/assignment_identifying_bias_in_ai_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE9r67nfPzgf"
      },
      "source": [
        "**This notebook is an exercise in the [AI Ethics](https://www.kaggle.com/learn/ai-ethics) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/identifying-bias-in-ai).**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.014195,
          "end_time": "2020-10-30T19:06:15.040067",
          "exception": false,
          "start_time": "2020-10-30T19:06:15.025872",
          "status": "completed"
        },
        "tags": [],
        "id": "oBD66r3xPzgh"
      },
      "source": [
        "In the tutorial, you learned about six different types of bias.  In this exercise, you'll train a model with **real data** and get practice with identifying bias.  Don't worry if you're new to coding: you'll still be able to complete the exercise!\n",
        "\n",
        "# Introduction\n",
        "\n",
        "At the end of 2017, the [Civil Comments](https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d) platform shut down and released their ~2 million public comments in a lasting open archive. Jigsaw sponsored this effort and helped to comprehensively annotate the data.  In 2019, Kaggle held the [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview) competition so that data scientists worldwide could work together to investigate ways to mitigate bias.\n",
        "\n",
        "The code cell below loads some of the data from the competition.  We'll work with thousands of comments, where each comment is labeled as either \"toxic\" or \"not toxic\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-30T19:06:15.082181Z",
          "iopub.status.busy": "2020-10-30T19:06:15.081291Z",
          "iopub.status.idle": "2020-10-30T19:06:45.746367Z",
          "shell.execute_reply": "2020-10-30T19:06:45.747005Z"
        },
        "papermill": {
          "duration": 30.692706,
          "end_time": "2020-10-30T19:06:45.747191",
          "exception": false,
          "start_time": "2020-10-30T19:06:15.054485",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1gQt9ixPzgh",
        "outputId": "0be1f436-81bd-4a0f-9059-ceb13f16f483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully loaded!\n",
            "\n",
            "Sample toxic comment: Too dumb to even answer.\n",
            "Sample not-toxic comment: No they aren't.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Get the same results each time\n",
        "np.random.seed(0)\n",
        "\n",
        "# Load the training data\n",
        "data = pd.read_csv(\"/content/data.csv\")\n",
        "comments = data[\"comment_text\"]\n",
        "target = (data[\"target\"]>0.7).astype(int)\n",
        "\n",
        "# Break into training and test sets\n",
        "comments_train, comments_test, y_train, y_test = train_test_split(comments, target, test_size=0.30, stratify=target)\n",
        "\n",
        "# Get vocabulary from training data\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(comments_train)\n",
        "\n",
        "# Get word counts for training and test sets\n",
        "X_train = vectorizer.transform(comments_train)\n",
        "X_test = vectorizer.transform(comments_test)\n",
        "\n",
        "# Preview the dataset\n",
        "print(\"Data successfully loaded!\\n\")\n",
        "print(\"Sample toxic comment:\", comments_train.iloc[22])\n",
        "print(\"Sample not-toxic comment:\", comments_train.iloc[17])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.012727,
          "end_time": "2020-10-30T19:06:45.774334",
          "exception": false,
          "start_time": "2020-10-30T19:06:45.761607",
          "status": "completed"
        },
        "tags": [],
        "id": "o_MiLpoEPzgj"
      },
      "source": [
        "Run the next code cell without changes to use the data to train a simple model.  The output shows the accuracy of the model on some test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-30T19:06:45.812366Z",
          "iopub.status.busy": "2020-10-30T19:06:45.811417Z",
          "iopub.status.idle": "2020-10-30T19:06:56.836497Z",
          "shell.execute_reply": "2020-10-30T19:06:56.835257Z"
        },
        "papermill": {
          "duration": 11.047454,
          "end_time": "2020-10-30T19:06:56.836704",
          "exception": false,
          "start_time": "2020-10-30T19:06:45.78925",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-mpyOWqPzgj",
        "outputId": "cfc6f924-0567-471c-9d86-bfc3bb21c078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9304022588097246\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train a model and evaluate performance on test dataset\n",
        "classifier = LogisticRegression(max_iter=2000)\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "print(\"Accuracy:\", score)\n",
        "\n",
        "# Function to classify any string\n",
        "def classify_string(string, investigate=False):\n",
        "    prediction = classifier.predict(vectorizer.transform([string]))[0]\n",
        "    if prediction == 0:\n",
        "        print(\"NOT TOXIC:\", string)\n",
        "    else:\n",
        "        print(\"TOXIC:\", string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.014849,
          "end_time": "2020-10-30T19:06:56.868884",
          "exception": false,
          "start_time": "2020-10-30T19:06:56.854035",
          "status": "completed"
        },
        "tags": [],
        "id": "_XuC0KfBPzgj"
      },
      "source": [
        "Roughly 93% of the comments in the test data are classified correctly!\n",
        "\n",
        "# 1) Try out the model\n",
        "\n",
        "You'll use the next code cell to write your own comments and supply them to the model: does the model classify them as toxic?  \n",
        "\n",
        "1. Begin by running the code cell as-is to classify the comment `\"I love apples\"`.  You should see that was classified as \"NOT TOXIC\".\n",
        "\n",
        "2. Then, try out another comment: `\"Apples are stupid\"`.  To do this, change only `\"I love apples\"` and leaving the rest of the code as-is.  Make sure that your comment is enclosed in quotes, as below.\n",
        "```python\n",
        "my_comment = \"Apples are stupid\"\n",
        "```\n",
        "3. Try out several comments (not necessarily about apples!), to see how the model performs: does it perform as suspected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-30T19:06:56.903202Z",
          "iopub.status.busy": "2020-10-30T19:06:56.902197Z",
          "iopub.status.idle": "2020-10-30T19:06:56.908224Z",
          "shell.execute_reply": "2020-10-30T19:06:56.908826Z"
        },
        "papermill": {
          "duration": 0.026189,
          "end_time": "2020-10-30T19:06:56.90901",
          "exception": false,
          "start_time": "2020-10-30T19:06:56.882821",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN15m-i0Pzgj",
        "outputId": "6c63a123-6268-4445-87d4-8ed134b4d61b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOXIC: Apples are stupid\n"
          ]
        }
      ],
      "source": [
        "# Comment to pass through the model\n",
        "my_comment = \"Apples are stupid\"\n",
        "\n",
        "# Do not change the code below\n",
        "classify_string(my_comment)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A: Yes the model performed as expected."
      ],
      "metadata": {
        "id": "LlM6qeDOSR7g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.018539,
          "end_time": "2020-10-30T19:06:56.942426",
          "exception": false,
          "start_time": "2020-10-30T19:06:56.923887",
          "status": "completed"
        },
        "tags": [],
        "id": "WPdSY8VyPzgj"
      },
      "source": [
        "Once you're done with testing comments, we'll move on to understand how the model makes decisions.  Run the next code cell without changes.\n",
        "\n",
        "The model assigns each of roughly 58,000 words a coefficient, where higher coefficients denote words that the model thinks are more toxic.  The code cell outputs the ten words that are considered most toxic, along with their coefficients.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-30T19:06:57.046892Z",
          "iopub.status.busy": "2020-10-30T19:06:57.037102Z",
          "iopub.status.idle": "2020-10-30T19:06:57.076223Z",
          "shell.execute_reply": "2020-10-30T19:06:57.075432Z"
        },
        "papermill": {
          "duration": 0.115908,
          "end_time": "2020-10-30T19:06:57.07637",
          "exception": false,
          "start_time": "2020-10-30T19:06:56.960462",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "_uLzMKxcPzgk",
        "outputId": "b2c5a049-4282-4666-a757-f3fb34eb4d7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            word     coeff\n",
              "20745      fools  6.276828\n",
              "34211      moron  6.330934\n",
              "16844       dumb  6.359265\n",
              "12907       crap  6.490039\n",
              "38317   pathetic  6.553948\n",
              "25850    idiotic  7.004129\n",
              "49802  stupidity  7.553888\n",
              "25858     idiots  8.601792\n",
              "25847      idiot  8.605594\n",
              "49789     stupid  9.279211"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30c9ade6-c5af-438f-a388-db796c20eb89\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>coeff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20745</th>\n",
              "      <td>fools</td>\n",
              "      <td>6.276828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34211</th>\n",
              "      <td>moron</td>\n",
              "      <td>6.330934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16844</th>\n",
              "      <td>dumb</td>\n",
              "      <td>6.359265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12907</th>\n",
              "      <td>crap</td>\n",
              "      <td>6.490039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38317</th>\n",
              "      <td>pathetic</td>\n",
              "      <td>6.553948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25850</th>\n",
              "      <td>idiotic</td>\n",
              "      <td>7.004129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49802</th>\n",
              "      <td>stupidity</td>\n",
              "      <td>7.553888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25858</th>\n",
              "      <td>idiots</td>\n",
              "      <td>8.601792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25847</th>\n",
              "      <td>idiot</td>\n",
              "      <td>8.605594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49789</th>\n",
              "      <td>stupid</td>\n",
              "      <td>9.279211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30c9ade6-c5af-438f-a388-db796c20eb89')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30c9ade6-c5af-438f-a388-db796c20eb89 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30c9ade6-c5af-438f-a388-db796c20eb89');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "coefficients = pd.DataFrame({\"word\": sorted(list(vectorizer.vocabulary_.keys())), \"coeff\": classifier.coef_[0]})\n",
        "coefficients.sort_values(by=['coeff']).tail(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.015335,
          "end_time": "2020-10-30T19:06:57.106908",
          "exception": false,
          "start_time": "2020-10-30T19:06:57.091573",
          "status": "completed"
        },
        "tags": [],
        "id": "bqwRj-WtPzgk"
      },
      "source": [
        "# 2) Most toxic words\n",
        "\n",
        "Take a look at the most toxic words from the code cell above.  Are you surprised to see any of them?  Are there any words that seem like they should not be in the list?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A: The list looks valid, no surprises."
      ],
      "metadata": {
        "id": "pmzs8n2WSq_S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.015333,
          "end_time": "2020-10-30T19:06:57.175887",
          "exception": false,
          "start_time": "2020-10-30T19:06:57.160554",
          "status": "completed"
        },
        "tags": [],
        "id": "GjuAV1oOPzgk"
      },
      "source": [
        "# 3) A closer investigation\n",
        "\n",
        "We'll take a closer look at how the model classifies comments.\n",
        "1. Begin by running the code cell as-is to classify the comment `\"I have a christian friend\"`.  You should see that was classified as \"NOT TOXIC\".  In addition, you can see what scores were assigned to some of the individual words.  Note that all words in the comment likely won't appear.\n",
        "2. Next, try out another comment: `\"I have a muslim friend\"`.  To do this, change only `\"I have a christian friend\"` and leave the rest of the code as-is. Make sure that your comment is enclosed in quotes, as below.\n",
        "```python\n",
        "new_comment = \"I have a muslim friend\"\n",
        "```\n",
        "3. Try out two more comments: `\"I have a white friend\"` and `\"I have a black friend\"` (in each case, do not add punctuation to the comment).\n",
        "4. Feel free to try out more comments, to see how the model classifies them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-30T19:06:57.230782Z",
          "iopub.status.busy": "2020-10-30T19:06:57.229963Z",
          "iopub.status.idle": "2020-10-30T19:06:57.235775Z",
          "shell.execute_reply": "2020-10-30T19:06:57.236398Z"
        },
        "papermill": {
          "duration": 0.044861,
          "end_time": "2020-10-30T19:06:57.236559",
          "exception": false,
          "start_time": "2020-10-30T19:06:57.191698",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "cIhtV8YnPzgk",
        "outputId": "b8b7720a-82aa-48ad-c385-26078171ea25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOXIC: I have a muslim friend\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         word     coeff\n",
              "21256  friend -0.130731\n",
              "24049    have -0.072306\n",
              "34692  muslim  1.767956"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94a5ff17-b66b-4633-8354-c925c16ea4d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>coeff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21256</th>\n",
              "      <td>friend</td>\n",
              "      <td>-0.130731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24049</th>\n",
              "      <td>have</td>\n",
              "      <td>-0.072306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34692</th>\n",
              "      <td>muslim</td>\n",
              "      <td>1.767956</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94a5ff17-b66b-4633-8354-c925c16ea4d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-94a5ff17-b66b-4633-8354-c925c16ea4d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-94a5ff17-b66b-4633-8354-c925c16ea4d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Set the value of new_comment\n",
        "new_comment = \"I have a muslim friend\"\n",
        "\n",
        "# Do not change the code below\n",
        "classify_string(new_comment)\n",
        "coefficients[coefficients.word.isin(new_comment.split())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.016152,
          "end_time": "2020-10-30T19:06:57.26916",
          "exception": false,
          "start_time": "2020-10-30T19:06:57.253008",
          "status": "completed"
        },
        "tags": [],
        "id": "nyD0bvUPPzgk"
      },
      "source": [
        "# 4) Identify bias\n",
        "\n",
        "Do you see any signs of potential bias in the model?  In the code cell above,\n",
        "- How did the model classify `\"I have a christian friend\"` and `\"I have a muslim friend\"`?  \n",
        "- How did it classify `\"I have a white friend\"` and `\"I have a black friend\"`?    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A: Clearly a bias here since the Christian and White were NON-TOXIC while the Muslim and Black were identified as TOXIC"
      ],
      "metadata": {
        "id": "-z1NaXKUS7wl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.016002,
          "end_time": "2020-10-30T19:06:57.342755",
          "exception": false,
          "start_time": "2020-10-30T19:06:57.326753",
          "status": "completed"
        },
        "tags": [],
        "id": "XmWpQYXvPzgk"
      },
      "source": [
        "# 5) Test your understanding\n",
        "\n",
        "We'll step away from the Jigsaw competition data and consider a similar (but hypothetical!) scenario where you're working with a dataset of online comments to train a model to classify comments as toxic.\n",
        "\n",
        "You notice that comments that refer to Islam are more likely to be toxic than comments that refer to other religions, because the online community is islamophobic.  What type of bias can this introduce to your model?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT0gz7smPzgk"
      },
      "source": [
        "# 6) Test your understanding, part 2\n",
        "\n",
        "We'll continue with the same hypothetical scenario, where you're trying to train a model to classify online comments as toxic.\n",
        "\n",
        "You take any comments that are not already in English and translate them to English with a separate tool.  Then, you treat all posts as if they were originally expressed in English.  What type of bias will your model suffer from?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A: I would say in this case this could fall into the measurement bias as the toxic or non-toxic words in a foreign language might not have the same meaning in English being a foreign non-toxic word might be translated into a toxic word in English. "
      ],
      "metadata": {
        "id": "E3jMZyUWWT46"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVso24q_Pzgk"
      },
      "source": [
        "# 7) Test your understanding, part 3\n",
        "\n",
        "We'll continue with the same hypothetical scenario, where you're trying to train a model to classify online comments as toxic.\n",
        "\n",
        "The dataset you're using to train the model contains comments primarily from users based in the United Kingdom.  \n",
        "\n",
        "After training a model, you evaluate its performance with another dataset of comments, also primarily from users based in the United Kingdom -- and it gets great performance!  You deploy it for a company based in Australia, and it does not perform well, because of differences between British and Australian English.  What types of bias does the model suffer from?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A: Primarly I would set this under the Representation bias as it is taking a dataset for a certain group and fitting it to another group that has a different set of values. It can also be part of the Measurement bias as words in the UK might have different conitations in Australia and thus the value of toxic vs. non-toxic might be different."
      ],
      "metadata": {
        "id": "lXXE187VXU9V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.016326,
          "end_time": "2020-10-30T19:06:57.417832",
          "exception": false,
          "start_time": "2020-10-30T19:06:57.401506",
          "status": "completed"
        },
        "tags": [],
        "id": "zFqrJg63Pzgl"
      },
      "source": [
        "# Learn more\n",
        "\n",
        "To continue learning about bias, check out the [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview) competition that was introduced in this exercise.  \n",
        "- Kaggler [Dieter](https://www.kaggle.com/christofhenkel) has written a helpful two-part series that teaches you how to preprocess the data and train a neural network to make a competition submission.  [Get started here](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda).\n",
        "- Many Kagglers have written helpful notebooks that you can use to get started.  [Check them out](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/notebooks?sortBy=voteCount&group=everyone&pageSize=20&competitionId=12500) on the competition page.\n",
        "\n",
        "Another Kaggle competition that you can use to learn about bias is the [Inclusive Images Challenge](https://www.kaggle.com/c/inclusive-images-challenge), which you can read more about in [this blog post](https://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html).  The competition focuses on **evaluation bias** in computer vision.\n",
        "\n",
        "# Keep going\n",
        "\n",
        "How can you quantify bias in machine learning applications?  Continue to **[learn how to measure fairness](https://www.kaggle.com/alexisbcook/ai-fairness)**."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}